Руководство по векторным эмбеддингам

Что такое эмбеддинги?

Эмбеддинги (embeddings) - это способ представления текста, слов или других данных в виде векторов чисел в многомерном пространстве. Главная идея заключается в том, что похожие по смыслу элементы должны быть расположены близко друг к другу в этом векторном пространстве.

Простая аналогия

Представьте карту города, где каждое здание - это слово. Похожие здания (например, все магазины) находятся в одном районе. Точно так же похожие слова имеют близкие координаты в векторном пространстве.

История эмбеддингов

Word2Vec (2013)
Революционный алгоритм от Google, который впервые показал, что векторная арифметика может отражать семантические отношения. Знаменитый пример: "король" - "мужчина" + "женщина" = "королева".

GloVe (2014)
Метод от Stanford, который использует глобальную статистику совместной встречаемости слов для создания эмбеддингов. GloVe показал хорошие результаты на задачах аналогий и классификации.

FastText (2016)
Улучшение от Facebook AI, которое учитывает структуру слов. В отличие от Word2Vec, FastText может генерировать эмбеддинги для слов, которых не было в обучающих данных.

ELMo (2018)
Первые контекстные эмбеддинги. Одно и то же слово может иметь разные эмбеддинги в зависимости от контекста. Например, "замок" (дверной) и "замок" (дворец) получат разные представления.

BERT (2018)
Трансформерная модель от Google, которая создает глубокие контекстные эмбеддинги. BERT учитывает контекст с обеих сторон слова (bidirectional), что дает более точные представления.

Sentence-BERT (2019)
Модификация BERT для создания эмбеддингов целых предложений. Эта модель специально оптимизирована для задач семантического поиска и кластеризации.

Как создаются эмбеддинги?

Токенизация
Первый шаг - разбиение текста на токены (обычно слова или подслова). Современные модели используют алгоритмы типа Byte-Pair Encoding (BPE) или WordPiece.

Энкодинг
Токены проходят через нейронную сеть (обычно трансформер), которая преобразует их в векторы фиксированной длины. Каждый слой сети постепенно улучшает представление, добавляя контекстную информацию.

Пулинг
Для получения эмбеддинга всего текста (а не отдельных токенов) применяются техники пулинга: mean pooling (усреднение), max pooling (максимум) или использование специального CLS токена.

Размерность эмбеддингов

Эмбеддинги - это векторы чисел определенной длины:
- Word2Vec: обычно 100-300 измерений
- BERT base: 768 измерений
- BERT large: 1024 измерений
- Sentence-BERT: часто 384 или 768 измерений
- OpenAI ada-002: 1536 измерений

Больше измерений = больше информации, но и больше вычислений.

Свойства эмбеддингов

Семантическая близость
Тексты с похожим смыслом имеют похожие эмбеддинги. "Собака" и "щенок" будут ближе друг к другу, чем "собака" и "компьютер".

Векторная арифметика
Можно выполнять математические операции над эмбеддингами для отражения семантических отношений.

Масштабируемость
Эмбеддинги позволяют эффективно работать с большими объемами текста, превращая проблему поиска по тексту в задачу поиска ближайших соседей в векторном пространстве.

Метрики сходства

Косинусное сходство
Самая популярная метрика. Измеряет косинус угла между векторами. Значения от -1 (противоположные) до 1 (идентичные).

Формула: cos(θ) = (A · B) / (||A|| × ||B||)

Евклидово расстояние
Прямое расстояние между точками в пространстве. Меньше значение = больше похожесть.

Формула: d = √(Σ(ai - bi)²)

Скалярное произведение
Простое произведение соответствующих компонентов векторов. Часто используется после нормализации векторов.

Применение эмбеддингов

Семантический поиск
Вместо поиска точных совпадений слов, система ищет тексты с похожим смыслом. Запрос "как приготовить пасту" найдет документы про "рецепты макарон".

Кластеризация
Группировка похожих документов без предварительной разметки. Например, автоматическая категоризация статей новостей.

Рекомендательные системы
Поиск похожих товаров, фильмов или контента на основе описаний и отзывов.

Детекция дубликатов
Нахождение почти одинаковых документов или текстов, даже если они написаны по-разному.

Классификация текстов
Эмбеддинги используются как признаки для ML моделей классификации (спам/не спам, позитивный/негативный отзыв).

Вопросно-ответные системы
RAG системы используют эмбеддинги для поиска релевантных фрагментов документов для ответа на вопросы.

Популярные модели для эмбеддингов

Для английского языка:
- all-MiniLM-L6-v2 (быстрая, 384 измерения)
- all-mpnet-base-v2 (качественная, 768 измерений)
- text-embedding-ada-002 (OpenAI)

Для русского и мультиязычные:
- paraphrase-multilingual-MiniLM-L12-v2
- paraphrase-multilingual-mpnet-base-v2
- LaBSE (Language-agnostic BERT Sentence Embedding)

Практические советы

Выбор модели
Для русского языка используйте мультиязычные модели. Для только английского языка можно взять специализированные английские модели - они быстрее и точнее.

Нормализация
Часто эмбеддинги нормализуются до единичной длины. Это упрощает сравнение и позволяет использовать скалярное произведение вместо косинусного сходства.

Батчинг
Обрабатывайте тексты батчами (например, по 32 или 64 текста), а не по одному. Это значительно ускоряет создание эмбеддингов.

Кэширование
Эмбеддинги для одного и того же текста всегда одинаковые (для одной модели). Сохраняйте их, чтобы не пересчитывать.

Качество данных
Предобработка текста (удаление HTML тегов, нормализация пробелов) улучшает качество эмбеддингов.

Ограничения эмбеддингов

Фиксированная длина
Большинство моделей имеют ограничение на длину входного текста (обычно 512 токенов). Длинные тексты нужно разбивать на части.

Зависимость от обучающих данных
Эмбеддинги отражают паттерны, которые модель видела при обучении. Специфическая терминология может быть представлена плохо.

Вычислительные ресурсы
Создание эмбеддингов требует GPU для больших объемов данных. Для миллионов документов это может быть дорого.

Обновление
При изменении документов нужно пересоздавать эмбеддинги. Для динамически изменяющихся данных это может быть проблемой.

Заключение

Эмбеддинги - это фундаментальная технология современного NLP и основа для RAG систем. Понимание их работы помогает строить более эффективные системы поиска и обработки текста. С развитием моделей качество эмбеддингов постоянно улучшается, открывая новые возможности для приложений.
