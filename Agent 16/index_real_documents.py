"""
–ò–ù–î–ï–ö–°–ê–¶–ò–Ø –†–ï–ê–õ–¨–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í
–ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ 'documents' –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∏—Ö
"""

from sentence_transformers import SentenceTransformer
import json
import numpy as np
import os
from pathlib import Path

def split_into_chunks(text, chunk_size=300, overlap=50):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
        if end < len(text):
            last_period = max(chunk.rfind('. '), chunk.rfind('! '), chunk.rfind('? '))
            if last_period > chunk_size * 0.5:
                chunk = chunk[:last_period + 1]
                end = start + last_period + 1
        
        if chunk.strip():
            chunks.append(chunk.strip())
        
        start = end - overlap
    
    return chunks


def load_documents_from_folder(folder_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏"""
    documents = []
    
    print(f"\nüìÇ –°–∫–∞–Ω–∏—Ä—É–µ–º –ø–∞–ø–∫—É: {folder_path}")
    
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
    text_extensions = ['.txt', '.md', '.py', '.js', '.html', '.css', '.json', '.csv', '.rst']
    
    folder = Path(folder_path)
    
    if not folder.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
        return documents
    
    # –ò—â–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    for file_path in folder.rglob('*'):
        if file_path.is_file() and file_path.suffix.lower() in text_extensions:
            print(f"   üìÑ –ù–∞–π–¥–µ–Ω: {file_path.name}")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                documents.append({
                    'filename': file_path.name,
                    'path': str(file_path),
                    'content': content
                })
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path.name}: {e}")
    
    return documents


def main():
    print("=" * 70)
    print("  –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –î–û–ö–£–ú–ï–ù–¢–û–í - –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏")
    print("=" * 70)
    
    # –ù–ê–°–¢–†–û–ô–ö–ò - –∏–∑–º–µ–Ω–∏—Ç–µ –ø–æ–¥ —Å–µ–±—è
    DOCUMENTS_FOLDER = "documents"  # –ü–∞–ø–∫–∞ —Å –≤–∞—à–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    INDEX_FILE = "document_index.json"  # –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å
    CHUNK_SIZE = 300  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    if not os.path.exists(DOCUMENTS_FOLDER):
        print(f"\nüìÅ –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É: {DOCUMENTS_FOLDER}")
        os.makedirs(DOCUMENTS_FOLDER)
        
        # –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–º–µ—Ä —Ñ–∞–π–ª–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        print("üìù –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤...")
        
        with open(f"{DOCUMENTS_FOLDER}/README.md", "w", encoding="utf-8") as f:
            f.write("""# –ú–æ–π AI –ü—Ä–æ–µ–∫—Ç

–≠—Ç–æ –ø—Ä–æ–µ–∫—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

## –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

- Python 3.8+
- Sentence Transformers
- FAISS –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
""")
        
        with open(f"{DOCUMENTS_FOLDER}/article.txt", "w", encoding="utf-8") as f:
            f.write("""–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ

–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å—ë –±–æ–ª–µ–µ –≤–∞–∂–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–µ–π. 
–û–Ω–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º —É—á–∏—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.

–û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
- Supervised Learning (–æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º)
- Unsupervised Learning (–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è)  
- Reinforcement Learning (–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º)

–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤–µ–∑–¥–µ: –æ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö 
—Å–∏—Å—Ç–µ–º –¥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π.
""")
        
        with open(f"{DOCUMENTS_FOLDER}/code_example.py", "w", encoding="utf-8") as f:
            f.write("""# –ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö

import pandas as pd

def load_data(file_path):
    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞\"\"\"
    df = pd.read_csv(file_path)
    return df

def preprocess_data(df):
    \"\"\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\"\"\"
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    df = df.dropna()
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
    df = (df - df.mean()) / df.std()
    return df
""")
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω—ã –ø—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ '{DOCUMENTS_FOLDER}'")
        print(f"   –í—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ–Ω–∏—Ç—å –∏—Ö —Å–≤–æ–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏!")
    
    # –®–ê–ì 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    documents = load_documents_from_folder(DOCUMENTS_FOLDER)
    
    if not documents:
        print("\n‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏!")
        print(f"   –î–æ–±–∞–≤—å—Ç–µ .txt, .md, .py –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É '{DOCUMENTS_FOLDER}'")
        return
    
    print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nüì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    
    # –®–ê–ì 3: –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
    print(f"\n‚úÇÔ∏è  –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä: {CHUNK_SIZE} —Å–∏–º–≤–æ–ª–æ–≤)...")
    
    all_chunks = []
    
    for doc_id, doc in enumerate(documents):
        chunks = split_into_chunks(doc['content'], chunk_size=CHUNK_SIZE)
        print(f"   {doc['filename']}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        for chunk_id, chunk_text in enumerate(chunks):
            all_chunks.append({
                'doc_id': doc_id,
                'chunk_id': chunk_id,
                'filename': doc['filename'],
                'text': chunk_text
            })
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print("\nüî¢ –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤...")
    
    chunk_texts = [chunk['text'] for chunk in all_chunks]
    embeddings = model.encode(chunk_texts, show_progress_bar=True)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ: {embeddings.shape[1]} —á–∏—Å–µ–ª")
    
    # –®–ê–ì 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ —Ñ–∞–π–ª: {INDEX_FILE}")
    
    index_data = {
        'documents': [
            {'filename': doc['filename'], 'path': doc['path']}
            for doc in documents
        ],
        'chunks': all_chunks,
        'embeddings': embeddings.tolist(),
        'config': {
            'chunk_size': CHUNK_SIZE,
            'model': "paraphrase-multilingual-MiniLM-L12-v2",
            'embedding_dim': embeddings.shape[1]
        }
    }
    
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    file_size = os.path.getsize(INDEX_FILE) / 1024  # KB
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω!")
    print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} KB")
    
    # –®–ê–ì 6: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞
    print("\n" + "=" * 70)
    print("  –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–ò–°–ö–ê")
    print("=" * 70)
    
    test_queries = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
        "–ö–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ?",
        "–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞"
    ]
    
    for query in test_queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
        
        # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = model.encode([query])[0]
        
        # –°—á–∏—Ç–∞–µ–º –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å–æ –≤—Å–µ–º–∏ —á–∞–Ω–∫–∞–º–∏
        similarities = []
        for emb in embeddings:
            similarity = np.dot(query_embedding, emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(emb)
            )
            similarities.append(similarity)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-2 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö
        top_indices = np.argsort(similarities)[-2:][::-1]
        
        for rank, idx in enumerate(top_indices, 1):
            chunk = all_chunks[idx]
            similarity = similarities[idx]
            
            print(f"\n   {rank}. –§–∞–π–ª: {chunk['filename']}")
            print(f"      –ü–æ—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f}")
            print(f"      –¢–µ–∫—Å—Ç: {chunk['text'][:120]}...")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
    print("\n" + "=" * 70)
    print("  ‚úÖ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 70)
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
    print(f"   ‚Ä¢ –ß–∞–Ω–∫–æ–≤: {len(all_chunks)}")
    print(f"   ‚Ä¢ –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}")
    print(f"   ‚Ä¢ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {INDEX_FILE}")
    print(f"\nüìÅ –í–∞—à–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–∞–ø–∫–µ: {DOCUMENTS_FOLDER}")
    print(f"üíæ –ò–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ RAG —Å–∏—Å—Ç–µ–º–µ!")


if __name__ == "__main__":
    main()
